{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution of Language Models: N-Grams, Word Embeddings, Attention & Transformers\n",
    "- based on https://towardsdatascience.com/evolution-of-language-models-n-grams-word-embeddings-attention-transformers-a688151825d2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before 1948-1980— Birth of N-Grams and Rule Systems\n",
    "\n",
    "By and large, majority of the NLP systems in this period were based on rules and the first few language models came in the form of N-Grams.\n",
    "\n",
    "It’s unclear from my research who coined this term.\n",
    "\n",
    "However, the first references of N-Grams came from Claude Shannon’s paper “A Mathematical Theory of Communications” published in 1948.\n",
    "\n",
    "Shannon references N-Grams a total of 3 times in this paper.\n",
    "\n",
    "This meant that the concept of N-Grams was probably formulated before 1948 by someone else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1980-1990 — Rise of compute power and the Birth of RNN\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*Jqfn9m-2wC5pfLpcwzk2Ug.png)\n",
    "A diagram for a one-unit recurrent neural network (RNN), 19 June 2017 by fdeloche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this decade, majority of the NLP research focused on statistical models capable of making probabilistic decisions.\n",
    "\n",
    "In 1982, John Hopfield introduced the Recurrent Neural Network (RNN) to be used for operations on sequence data i.e. text or voice\n",
    "\n",
    "By 1986, the first ideas of representing words as vectors emerged. These studies were conducted by Geoffrey Hinton, one of the Godfathers of modern day AI research.(Hinton et al. 1986; Rumelhart et al. 1986)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1990-2000— The Rise of NLP Research and the Birth of LSTM\n",
    "![](https://miro.medium.com/max/1400/1*-4NZcM5LFHnf7Xp0Cyeu0Q.png)\n",
    "A diagram for a one-unit Long Short-Term Memory (LSTM), 20 June 2017 by fdeloche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 1990s, NLP analysis began to grow in popularity.\n",
    "\n",
    "N-Grams became extremely useful in making sense of textual data.\n",
    "\n",
    "By 1997, the idea of Long Short Term Memory networks (LSTM) was introduced by Hochreiter et al. (1997).\n",
    "\n",
    "However, there was still a lack of compute power in this period to fully utilize the neural language models to its fullest potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2003 — The First Neural Language Model\n",
    "\n",
    "In 2003, the very first feed-forward neural network language model was proposed by Bengio et al. (2003).\n",
    "\n",
    "Bengio et al. (2003) model consisted of a single hidden layer feed-forward network used to predict the next word of a sequence.\n",
    "\n",
    "![](https://miro.medium.com/max/1192/1*Nj81Dc6OpYT9G9kgT3DOrA.png)\n",
    "The first neural language model by Bengio et al. 2003 \n",
    "\n",
    "Although feature vectors already existed by this time, Bengio et al.(2003) were the ones that brought the concept to the masses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2013 — Birth of Widespread Pretrained Word Embeddings (Word2Vec by Google)\n",
    "\n",
    "In 2013, Google introduced Word2Vec. (Mikolov et al., 2013)\n",
    "\n",
    "The goal of Mikolov et al. (2013) was to introduce novel techniques to be able to learn high-quality word embeddings from huge corpora that were transferable across NLP applications.\n",
    "\n",
    "These techniques were the:\n",
    "- Continuous bag-of-words (CBOW) &\n",
    "- Skip-Gram\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*Kv0ZBh7nCi2TDDHJbmvBBw.png)\n",
    "Word2Vec models. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. By Mikolov et al. 2013 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of Mikolov et al. (2013) pretrained word embeddings paved the way for a multitude of NLP applications for years to come.\n",
    "\n",
    "Till this day, people still use pretrained word embeddings for various NLP applications.\n",
    "\n",
    "It was in this period that LSTMs, RNNs and Gated Recurrent Units (GRU) started to be widely adopted for many different NLP applications as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2014 — Standford: Global Vectors (Glove)\n",
    "\n",
    "A year after Word2Vec was introduced, Pennington et al. (2014) from Standford University presented Glove.\n",
    "\n",
    "Glove was a set of pretrained word embeddings trained on a different set of corpora with a different technique.\n",
    "\n",
    "Pennington et al. (2014) found that word embeddings could be learned by co-occurrence matrices and proved that their method could outperform Word2Vec on word similarity tasks and Named Entity Recognition (NER).\n",
    "![](https://miro.medium.com/max/1400/1*kvKfEwc-Dpq6MMzoEofaxw.png)\n",
    "Overall accuracy on the word analogy task Glove vs CBOW vs Skip-Gram by Pennington et al. 2014 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2015 — The Comeback: SVD and LSA Word Embeddings & The Birth of Attention Models\n",
    "\n",
    "Recent trends on neural network models were seemingly outperforming traditional models on word similarity and analogy detection tasks.\n",
    "\n",
    "It was here that researchers Levy et al. (2015) conducted a study on these trending methodologies to learn how they stacked up against the traditional statistical methods.\n",
    "\n",
    "Levy et al. (2015) found that with proper tuning, classic matrix factorization methods like SVD(Singular Value Decomposition) and LSA (Latent Semantic Analysis) attained similar results to Word2Vec or Glove.\n",
    "\n",
    "They concluded that there were insignificant performance differences between the old and new methods and that there was no evidence of an advantage to any single approach over the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Birth of the Attention Model\n",
    "\n",
    "In previous studies, the problem with Neural Machine Translation (NMT) with RNNs was that they tend to “forget” what was learnt if the sentences got too long.\n",
    "\n",
    "This was noted as the problem of “long-term dependencies”.\n",
    "\n",
    "As such, Bahdanau et al. (2015) proposed the attention mechanism to address this issue.\n",
    "\n",
    "Rather than having a model remember an entire input sequence before translation, the attention mechanism replicates how humans would go about a translation task.\n",
    "\n",
    "The mechanism allowed models to focus on only the words that best helped the model to translate a word correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2016 — From Neural Machine Translation to Image Captioning with Attention\n",
    "\n",
    "Shortly after the attention mechanism was proposed, other researchers like Xu et al. (2016) began studies on how to use attention in other NLP applications like Image Captioning.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*lpfua6ajFawoMWDCx8r4Zw.png)\n",
    "Examples of attending to the correct object (white indicates the attended regions, underlines indicated the corresponding word) by Xu et al. 2016\n",
    "\n",
    "Notice how the model “attends” to the correct areas of the images to caption the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017 — Birth of the Transformer\n",
    "\n",
    "Given the success of the attention mechanism in NLP applications, Google proposed a new simple network architecture known as the Transformer. (Vaswani et al., 2017)\n",
    "\n",
    "This was a paradigm shift from the standard way NLP applications were built upon. i.e. Using RNNs, LSTMs or GRUs initialized with word embeddings.\n",
    "\n",
    "Vaswani et al. (2017) based the Transformer solely on the attention mechanism which did away with RNNs entirely.\n",
    "\n",
    "![](https://miro.medium.com/max/924/1*sOqN2gebImOhGIfpoNKsHQ.png)\n",
    "The Transformer — model architecture by Vaswani et al. 2017\n",
    "\n",
    "Vaswani et al. (2017) work soon gave birth to the game-changing Bidirectional Encoder Representations from Transformers a.k.a BERT the next year. (Devlin et al., 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018-Today— Era of Pretrained Language Models\n",
    "\n",
    "As mentioned previously, the past 2 years showed a paradigm shift in the way NLP models were being built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Word Embeddings had evolved to Pretrained Language Models.\n",
    "\n",
    "The intuition here was to initialize an entire model architecture with pretrained weights instead of just the input layer of a model with pretrained word embeddings.\n",
    "\n",
    "It was found that pretrained language models only needed fine-tuning to be relatively high performing at various NLP tasks.\n",
    "\n",
    "Some examples of these pretrained architectures are, BERT, ALBERT, ROBERTA, ELMO, ERNIE, XLNET, GTP-2, T5 and more.\n",
    "\n",
    "As of today, a number of these models actually outperform BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Latest Breakthroughs and Developments in Natural Language Processing\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2019/08/complete-list-important-frameworks-nlp/?utm_source=blog&utm_medium=demystifying-bert-groundbreaking-nlp-framework\n",
    "\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/Latest-Breakthroughs-and-Developments-nlp-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From BERT to ALBERT\n",
    "\n",
    "https://medium.com/doxastar/from-bert-to-albert-pre-trained-langaug-models-5865aa5c3762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer-Based Pre-tained Models\n",
    "\n",
    "GPT (OpenAI Transformer) is the first Transformer_based pre-trained language model, it uses the decoder of the Transformer to model the language as it is an autoregressive model where the model predicts the next word according to its previous context. GPT has shown a good performance on many downstream tasks.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*corMthPJwan-yw0KOcZ6qQ.png)\n",
    "The timeline of emerging almost all pre-trained language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
